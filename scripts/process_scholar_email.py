import email
import logging
import os
import re
import base64
import json
from email.header import decode_header
from time import sleep

from bs4 import BeautifulSoup
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build

from .common_utils import (
    clean_google_url,
    fetch_article_body,
    is_duplicate_md,
    safe_filename,
    translate_text,
    summarize_and_translate_body,
)

# --- Constants ---
SCOPES = ["https://www.googleapis.com/auth/gmail.modify"]
TOKEN_PATH = "token.json"
CREDENTIALS_PATH = "credentials.json"
SEEN_PAPERS_FILE = "seen_scholar_messages.json"

PAPERS_OUTPUT_DIR = os.path.join("docs", "keywords")
MIN_BODY_LENGTH = 300
API_RATE_LIMIT_DELAY = 1.5  # seconds


# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()],
)


def get_gmail_service():
    """Gmail API ì„œë¹„ìŠ¤ ê°ì²´ë¥¼ ì¸ì¦í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    creds = None
    if os.path.exists(TOKEN_PATH):
        creds = Credentials.from_authorized_user_file(TOKEN_PATH, SCOPES)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            if not os.path.exists(CREDENTIALS_PATH):
                logging.error(f"'{CREDENTIALS_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Google Cloudì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì£¼ì„¸ìš”.")
                return None
            flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)
            creds = flow.run_local_server(port=0)
        with open(TOKEN_PATH, "w") as token:
            token.write(creds.to_json())
    return build("gmail", "v1", credentials=creds)


def get_html_payload_from_message(msg):
    """Gmail API ë©”ì‹œì§€ ê°ì²´ì—ì„œ HTML payloadë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤."""
    if "parts" in msg["payload"]:
        for part in msg["payload"]["parts"]:
            if part["mimeType"] == "text/html":
                return part["body"].get("data")
            data = get_html_payload_from_message({"payload": part})
            if data:
                return data
    elif msg["payload"]["mimeType"] == "text/html":
        return msg["payload"]["body"].get("data")
    return None


def parse_scholar_email(msg):
    """Gmail ë©”ì‹œì§€ë¥¼ íŒŒì‹±í•˜ì—¬ í‚¤ì›Œë“œì™€ ë…¼ë¬¸ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    headers = {h["name"]: h["value"] for h in msg["payload"]["headers"]}
    subject = headers.get("Subject", "")
    keyword = subject.split(" - ")[0].strip()
    logging.info(f"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keyword}")

    # HTML ë³¸ë¬¸ ì¶”ì¶œ
    body_data = get_html_payload_from_message(msg)
    if not body_data:
        logging.error("ì´ë©”ì¼ì—ì„œ HTML ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return keyword, []

    html_content = base64.urlsafe_b64decode(body_data).decode("utf-8")
    if not html_content:
        logging.error("ì´ë©”ì¼ì—ì„œ HTML ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return keyword, []

    soup = BeautifulSoup(html_content, "html.parser")
    articles = []

    # ê° ë…¼ë¬¸ í•­ëª©ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    # Google ì´ë©”ì¼ í…œí”Œë¦¿ ë³€ê²½ì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ h3 íƒœê·¸ ëŒ€ì‹  a íƒœê·¸ë¥¼ ì§ì ‘ ì°¾ë„ë¡ ìˆ˜ì •í•©ë‹ˆë‹¤.
    for link_tag in soup.find_all("a", class_="gse_alrt_title"):
        title_en = link_tag.get_text(strip=True)
        url = link_tag.get("href", "")

        if not title_en or not url:
            continue

        # ìŠ¤ë‹ˆí«(ìš”ì•½)ì„ ì°¾ìŠµë‹ˆë‹¤. a íƒœê·¸ì˜ ë¶€ëª¨(h3)ë¥¼ ì°¾ê³ , ê·¸ ë‹¤ìŒ í˜•ì œ ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        # ì´ êµ¬ì¡°ëŠ” ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ìŠ¤ë‹ˆí«ì„ ëª»ì°¾ì•„ë„ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        snippet = ""
        h3_parent = link_tag.find_parent("h3")
        if h3_parent:
            snippet_tag = h3_parent.find_next_sibling("div", class_="gse_alrt_sni")
            if snippet_tag:
                snippet = snippet_tag.get_text(strip=True)

        articles.append({"title_en": title_en, "url": url, "snippet": snippet})

    logging.info(f"ğŸ“„ ì´ë©”ì¼ì—ì„œ {len(articles)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return keyword, articles


def get_existing_titles(keyword):
    """í‚¤ì›Œë“œ ë””ë ‰í† ë¦¬ì—ì„œ ê¸°ì¡´ì— ì €ì¥ëœ ëª¨ë“  ë…¼ë¬¸ì˜ ì›ë³¸ ì œëª© setì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    titles = set()
    folder = os.path.join(PAPERS_OUTPUT_DIR, keyword)
    if not os.path.exists(folder):
        return titles

    for filename in os.listdir(folder):
        if filename.endswith(".md"):
            filepath = os.path.join(folder, filename)
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    content = f.read()
                    match = re.search(r"\*\*ì›ì œëª©:\*\*\s*(.*)", content)
                    if match:
                        titles.add(match.group(1).strip())
            except Exception as e:
                logging.warning(f"ê¸°ì¡´ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {filepath}: {e}")
    return titles


def save_paper_markdown(keyword, title_ko, title_en, summary_ko, url, existing_titles):
    """ë…¼ë¬¸ ìš”ì•½ì„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì¤‘ë³µì„ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        if title_en in existing_titles:
            logging.info(f"ğŸš« ì¤‘ë³µ ë…¼ë¬¸: {title_en}")
            return

        safe_title = safe_filename(title_ko)
        folder = os.path.join(PAPERS_OUTPUT_DIR, keyword)
        os.makedirs(folder, exist_ok=True)
        path = os.path.join(folder, f"{safe_title}.md")

        with open(path, "w", encoding="utf-8") as f:
            f.write(f"# {title_ko}\n\n")
            f.write(f"**ì›ì œëª©:** {title_en}\n\n")
            f.write(f"**ìš”ì•½:** {summary_ko}\n\n")
            f.write(f"[ì›ë¬¸ ë§í¬]({url})\n")
        logging.info(f"âœ… ì €ì¥ ì™„ë£Œ: {path}")
        existing_titles.add(title_en)  # ì²˜ë¦¬ëœ ëª©ë¡ì— ì¶”ê°€

    except Exception as e:
        logging.error(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({title_en}): {e}")


def load_seen_ids():
    """ì²˜ë¦¬ëœ ì´ë©”ì¼ ID ëª©ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    if os.path.exists(SEEN_PAPERS_FILE):
        with open(SEEN_PAPERS_FILE, "r") as f:
            return set(json.load(f))
    return set()


def save_seen_ids(ids):
    """ì²˜ë¦¬ëœ ì´ë©”ì¼ ID ëª©ë¡ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(SEEN_PAPERS_FILE, "w") as f:
        json.dump(list(ids), f)


def main():
    service = get_gmail_service()
    if not service:
        return

    # ì½ì§€ ì•Šì€ êµ¬ê¸€ ìŠ¤ì¹¼ë¼ ì•Œë¦¬ë¯¸ ë©”ì¼ë§Œ ê°€ì ¸ì˜¤ë„ë¡ ì¿¼ë¦¬ë¥¼ ë³µì›í•©ë‹ˆë‹¤.
    query = "from:scholaralerts-noreply@google.com is:unread"
    results = service.users().messages().list(userId="me", q=query).execute()
    messages = results.get("messages", [])

    if not messages:
        logging.info("ì²˜ë¦¬í•  ìƒˆ Google Scholar ì•Œë¦¬ë¯¸ ë©”ì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    logging.info(f"ì´ {len(messages)}ê°œì˜ ìƒˆ ì•Œë¦¬ë¯¸ ë©”ì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    seen_ids = load_seen_ids()

    for msg_info in messages:
        msg_id = msg_info["id"]
        if msg_id in seen_ids:
            continue

        msg = service.users().messages().get(userId="me", id=msg_id, format="full").execute()
        keyword, articles = parse_scholar_email(msg)
        if not articles:
            continue

        existing_titles = get_existing_titles(keyword)
        logging.info(f"ê¸°ì¡´ì— ì €ì¥ëœ '{keyword}' ë…¼ë¬¸ {len(existing_titles)}ê°œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")

        for article in articles:
            title_en, link_url, snippet = article["title_en"], article["url"], article["snippet"]
            logging.info(f"--- âš™ï¸ ì²˜ë¦¬ ì‹œì‘: {title_en} ---")

            link = clean_google_url(link_url)
            
            # 1. ë³¸ë¬¸ ì¶”ì¶œì„ ë¨¼ì € ì‹œë„í•©ë‹ˆë‹¤.
            body = fetch_article_body(link)

            # 2. ë³¸ë¬¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´, ì´ë©”ì¼ì˜ ìŠ¤ë‹ˆí«ì„ ëŒ€ì²´ì¬ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            if not body or len(body.strip()) < MIN_BODY_LENGTH:
                logging.info(f"  - â„¹ï¸ ì •ë³´: ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ë¶€ì¡±. ì´ë©”ì¼ ìŠ¤ë‹ˆí«ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                body = snippet

            # 3. ë³¸ë¬¸ê³¼ ìŠ¤ë‹ˆí«ì´ ëª¨ë‘ ë¹„ì–´ìˆìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
            if not body or not body.strip():
                logging.warning(f"ë³¸ë¬¸/ìŠ¤ë‹ˆí«ì´ ëª¨ë‘ ë¹„ì–´ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤: {title_en}")
                continue

            title_ko = translate_text(title_en)
            summary_ko = summarize_and_translate_body(body)
            if title_ko and summary_ko:
                save_paper_markdown(keyword, title_ko, title_en, summary_ko, link, existing_titles)
            else:
                logging.error(f"ë²ˆì—­ ì‹¤íŒ¨: {title_en}")
            sleep(API_RATE_LIMIT_DELAY)

        # ì²˜ë¦¬ê°€ ëë‚œ ë©”ì¼ì€ 'ì½ìŒ'ìœ¼ë¡œ í‘œì‹œí•˜ê³ , seen ëª©ë¡ì— ì¶”ê°€
        service.users().messages().modify(userId="me", id=msg_id, body={"removeLabelIds": ["UNREAD"]}).execute()
        seen_ids.add(msg_id)

    save_seen_ids(seen_ids)


if __name__ == "__main__":
    main()