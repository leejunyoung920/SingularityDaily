# collect_from_gmail.py (RSS ë°©ì‹ìœ¼ë¡œ ê°œì„ )
import os
import feedparser
import logging
from datetime import datetime
from .common_utils import (
from concurrent.futures import ThreadPoolExecutor, as_completed
    clean_google_url,
    strip_html_tags,
    fetch_article_body,
    safe_filename,
    is_duplicate_md,
    translate_text,
    summarize_and_translate_body,
    initialize_gemini,
)

# --- Constants ---
RSS_FEEDS = {
    "AGI": "https://www.google.co.kr/alerts/feeds/14276058857012603250/2707178187233880419",
    "AI drug discovery": "https://www.google.co.kr/alerts/feeds/14276058857012603250/2271409061188943971",
    "Anti-aging therapeutics": "https://www.google.co.kr/alerts/feeds/14276058857012603250/1502131717617198121",
    "Cellular reprograming": "https://www.google.co.kr/alerts/feeds/14276058857012603250/2481308844339361893",
    "Longevity research": "https://www.google.co.kr/alerts/feeds/14276058857012603250/9706346182581700369",
    "nanobot": "https://www.google.co.kr/alerts/feeds/14276058857012603250/2271409061188945116",
    "NMN": "https://www.google.co.kr/alerts/feeds/14276058857012603250/1502131717617199599",
    "Rapamycin": "https://www.google.co.kr/alerts/feeds/14276058857012603250/2707178187233881309",
    "Senolytics": "https://www.google.co.kr/alerts/feeds/14276058857012603250/1502131717617200498",
    "Telomere extension": "https://www.google.co.kr/alerts/feeds/14276058857012603250/9135595537824711247",
    "Humanoid Robot": "https://www.google.co.kr/alerts/feeds/14276058857012603250/1273794955109409208"
}
MIN_BODY_LENGTH = 300
MAX_ENTRIES_PER_FEED = 20
API_RATE_LIMIT_DELAY = 1.5  # seconds
OUTPUT_DIR = os.path.join("docs", "keywords")

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

def save_markdown(keyword, title_ko, title_en, summary_ko, url):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì €ì¥í•˜ê³ , ì¤‘ë³µì„ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        safe_title = safe_filename(title_ko)
        folder = os.path.join(OUTPUT_DIR, keyword)
        os.makedirs(folder, exist_ok=True)
        path = os.path.join(folder, f"{safe_title}.md")

        if is_duplicate_md(path, title_en):
            logging.info(f"ğŸš« ì¤‘ë³µ ê¸°ì‚¬: {title_en}")
            return False

        with open(path, "w", encoding="utf-8") as f:
            f.write(f"# {title_ko}\n\n")
            f.write(f"**ì›ì œëª©:** {title_en}\n\n")
            f.write(f"**ìš”ì•½:** {summary_ko}\n\n")
            f.write(f"[ì›ë¬¸ ë§í¬]({url})\n")
        logging.info(f"âœ… ì €ì¥ ì™„ë£Œ: {path}")
        return True
    except Exception as e:
        logging.error(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({title_en}): {e}")
        return False

def process_entry(entry, keyword):
    """ê°œë³„ RSS í•­ëª©ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        raw_title = entry.get("title", "")
        raw_link = entry.get("link", "")
        link = clean_google_url(raw_link)
        title_en = strip_html_tags(raw_title)

        if not title_en or not link:
            logging.warning("ì œëª© ë˜ëŠ” ë§í¬ê°€ ì—†ëŠ” í•­ëª©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        # ë³¸ë¬¸ ë¨¼ì € ì¶”ì¶œ í›„ í•„í„°
        body = fetch_article_body(link)

        # ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬
        if not body:
            logging.info(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ â€” ì €ì¥í•˜ì§€ ì•ŠìŒ: {title_en}")
            return
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì„ ê²½ìš° ì²˜ë¦¬
        if len(body.strip()) < MIN_BODY_LENGTH:
            logging.info(f"âš ï¸ ë³¸ë¬¸ ë¶€ì¡±({len(body.strip())}ì) â€” ì €ì¥í•˜ì§€ ì•ŠìŒ: {title_en}")
            return

        # ë²ˆì—­ ìˆ˜í–‰ (ë¹„ìš© ë°œìƒ)
        title_ko = translate_text(title_en)
        summary_ko = summarize_and_translate_body(body)
        
        if not title_ko or not summary_ko:
            logging.error(f"ë²ˆì—­ ì‹¤íŒ¨: {title_en}")
            return

        save_markdown(keyword, title_ko, title_en, summary_ko, link)

    except Exception as e:
        logging.error(f"í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({entry.get('title', 'N/A')}): {e}")

def main():
    """ëª¨ë“  RSS í”¼ë“œë¥¼ ìˆœíšŒí•˜ë©° ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        initialize_gemini()
    except (ValueError, RuntimeError) as e:
        logging.error(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ë‹¨: {e}")
        # CI/CD í™˜ê²½ì—ì„œ ì‹¤íŒ¨ë¥¼ ëª…í™•íˆ ì•Œë¦¬ê¸° ìœ„í•´ 0ì´ ì•„ë‹Œ ì½”ë“œë¡œ ì¢…ë£Œ
        exit(1)

    all_tasks = []
    for keyword, feed_url in RSS_FEEDS.items():
        logging.info(f"========== ğŸŒ RSS í”¼ë“œ ìŠ¤ìº” ì¤‘: {keyword} ==========")
        try:
            feed = feedparser.parse(feed_url)
            if feed.bozo:
                # bozoê°€ 1ì´ë©´ í”¼ë“œ íŒŒì‹±ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒì„ ì˜ë¯¸
                logging.warning(f"'{keyword}' í”¼ë“œ íŒŒì‹± ë¬¸ì œ: {feed.bozo_exception}")

            entries = feed.entries[:MAX_ENTRIES_PER_FEED]
            for entry in entries:
                all_tasks.append((entry, keyword))
        except Exception as e:
            logging.error(f"'{keyword}' í”¼ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if not all_tasks:
        logging.info("ì²˜ë¦¬í•  ìƒˆ RSS í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    logging.info(f"ì´ {len(all_tasks)}ê°œì˜ RSS í•­ëª©ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")

    # max_workersë¥¼ 5ë¡œ ì„¤ì •í•˜ì—¬ ë™ì‹œì— 5ê°œì˜ í•­ëª©ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    # ì´ëŠ” API ì†ë„ ì œí•œì„ ì–´ëŠ ì •ë„ ì œì–´í•˜ëŠ” íš¨ê³¼ë„ ìˆìŠµë‹ˆë‹¤.
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_task = {executor.submit(process_entry, task[0], task[1]): task for task in all_tasks}
        for future in as_completed(future_to_task):
            task = future_to_task[future]
            try:
                future.result()  # ì‘ì—… ì¤‘ ë°œìƒí•œ ì˜ˆì™¸ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ ë‹¤ì‹œ ë°œìƒì‹œí‚µë‹ˆë‹¤.
            except Exception as exc:
                logging.error(f"í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ ({task[0].get('title', 'N/A')}): {exc}")

    logging.info("========== RSS ìˆ˜ì§‘ ì¢…ë£Œ ==========\n")

if __name__ == "__main__":
    main()